name: Backup Neon DB to Seohost

on:
  schedule:
    - cron: "0 2 * * 0"
  workflow_dispatch:

jobs:
  backup-and-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # 1. Dump using Docker
      - name: Create Backup with Docker
        env:
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          FILENAME="moodbox-backup-${TIMESTAMP}.sql.gz"
          
          echo "ðŸ—ƒ Dumping DB using Postgres 17 Container..."
          
          # We run a temporary docker container just to execute pg_dump
          # We mount the current directory ($(pwd)) to /backup so we can write the file out
          docker run --rm \
            -v $(pwd):/backup \
            postgres:17-alpine \
            /bin/sh -c "pg_dump '$POSTGRES_URL' --no-owner --no-privileges --clean --if-exists | gzip > /backup/$FILENAME"
            
          echo "BACKUP_FILE=$FILENAME" >> $GITHUB_ENV

      # 2. Upload (Your existing lftp logic is fine, though I still prefer SFTP)
      - name: Upload via FTPS
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          REMOTE_DIR: /domains/moodbox.pl/db_backups/
        run: |
          lftp -c "
            set ftp:ssl-auth TLS
            set ftp:ssl-force true
            set ftp:ssl-protect-data true
            set ssl:verify-certificate no
            open -u '$FTP_USER','$FTP_PASS' '$FTP_HOST';
            mkdir -p -f '$REMOTE_DIR';
            put -O '$REMOTE_DIR' '${{ env.BACKUP_FILE }}';
            bye
          "
